# What is the best LLM Chatbot in South Korea?
Code release of our paper [paper](https://drive.google.com/file/d/1a4qkD8U658KFXHW9UsBmhWSzECakImrD/view?usp=sharing).

## Abstract
![](images/pipeline.png)

>Hae Chan Kim*, Jonghyeok Shin*, Kanghyeon Kim* Seungwoo Lee*<br/>
>\* Denotes equal contribution <br/><br/>
>With the development of large language models (LLMs), many companies have released new chatbot services in South Korea. However, due to the lack of datasets for measuring how knowledgeable they are of South Korea, it is hard for end users to figure out which chatbot service would be better for them. Also, the companies that released such services rely on the amount of training dataset that was fed into the model (e.g. CLOVA X) or the scores of some tests (e.g. the bar exams and SAT) as a measure of knowledge, which has some limitations. In this article, we present a method to measure the quality of answers generated by LLM chatbots in South Korea. It employs the Fightinâ€™ Words method to count the number of marked words used when describing certain topics related to South Korea, which can be a measure of how knowledgeable the language models are of the topics, based on our premise. Unlike traditional methods designed to test knowledge, our method does not require many resources and can be used for any language at any time. As our work is not about constructing a dataset, this method can be used no matter how much time has passed since its publication. Through this method, we evaluate the knowledge of gpt-3.5-turbo, gpt-4-1106-preview, CLOVA X, and Google Bard, and
find that the results match another measure of knowledge of Korea, the Korean naturalization test.


## Results
This is the qualitative result presented in our paper.
![Qualitative](images/qualitative_result.png)

In the evaluation part, we compared results of MultiDreamer(Ours) and SyncDreamer(Baseline). We measured Chamfer Distance, Volume IoU, and F-Score for quantitative evaluation. The code to obtain results for both models and compute the metrics is in `eval/eval.sh`. You can run : 
```
$ conda env create -n eval -f ./env/eval.yaml
$ conda activate eval
$ cd eval
$ bash eval.sh
```

Finally, you can obtain the result like the table below. The values of each metric may differ from the table, as they are computed from randomly sampled vertices in the inferred mesh and the ground truth mesh.

![Quantitative](images/quantitative_result.png)


### Libraries
- https://konlpy.org/en/latest/
  
  
### Projects
- https://github.com/myracheng/markedpersonas

- https://github.com/jmhessel/FightingWords
